import asyncio
import json
import logging
import re
import urllib.parse
from typing import List, Optional, Dict, Any

import aiohttp
from bs4 import BeautifulSoup
from tenacity import retry, wait_fixed, stop_after_attempt, retry_if_exception_type, RetryError as TenacityRetryError
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.constants import ChatAction
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, MessageHandler, filters, ContextTypes

# Embedded credentials
TELEGRAM_BOT_TOKEN = "1951771121:AAEn0VTc-8Ejx_RToZl_i69W0z6NCrau4I0"
METIS_API_KEY = "tpsg-6WW8eb5cZfq6fZru3B6tUbSaKB2EkVm"
METIS_MODEL = "gpt-4o"
API_URL_METIS = "https://api.metis.ai/v1/chat/completions"

# Retry configuration
RETRY_ATTEMPTS = 3
RETRY_WAIT_SECONDS = 2

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RetryableError(Exception):
    pass

class ContentScraper:
    def __init__(self, session: aiohttp.ClientSession):
        self.session = session
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }

    async def fetch_page(self, url: str, timeout: int = 15, params: dict = None) -> str:
        """Fetch a web page with proper headers and error handling"""
        try:
            kwargs = {
                'headers': self.headers,
                'timeout': timeout,
                'ssl': False  # Disable SSL verification
            }
            
            if params:
                kwargs['params'] = params
            
            async with self.session.get(url, **kwargs) as response:
                if response.status == 200:
                    content = await response.text()
                    logger.debug(f"Successfully fetched {len(content)} characters from {url}")
                    return content
                else:
                    logger.warning(f"HTTP {response.status} for {url}")
                    return ""
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            return ""

    def clean_text(self, text: str) -> str:
        """Clean and normalize text content"""
        # Remove extra whitespace and newlines
        text = re.sub(r'\s+', ' ', text).strip()
        # Remove HTML entities
        text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
        return text

    def extract_text_from_html(self, html: str, selectors: List[str]) -> List[str]:
        """Extract text content using multiple CSS selectors"""
        if not html:
            return []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.decompose()
            
            results = []
            for selector in selectors:
                try:
                    elements = soup.select(selector)
                    for element in elements:
                        text = self.clean_text(element.get_text())
                        if text and len(text) > 20:  # Filter out very short texts
                            results.append(text)
                except Exception as e:
                    logger.debug(f"Selector {selector} failed: {e}")
                    continue
            
            return results
        except Exception as e:
            logger.error(f"HTML parsing error: {e}")
            return []

    async def search_duckduckgo(self, query: str) -> List[Dict[str, str]]:
        """Search using DuckDuckGo which is more reliable"""
        try:
            # DuckDuckGo instant answer API
            ddg_url = "https://api.duckduckgo.com/"
            params = {
                'q': query,
                'format': 'json',
                'no_html': '1',
                'skip_disambig': '1'
            }
            
            html = await self.fetch_page(ddg_url, params=params)
            if html:
                try:
                    data = json.loads(html)
                    abstract = data.get('AbstractText', '')
                    if abstract:
                        return [{'title': f'Ø®Ù„Ø§ØµÙ‡ {query}', 'snippet': abstract[:500], 'url': ''}]
                except:
                    pass
            
            # Search via HTML
            search_url = f"https://html.duckduckgo.com/html/?q={urllib.parse.quote(query)}"
            html = await self.fetch_page(search_url)
            
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            results = []
            
            # Extract search results from DuckDuckGo
            search_results = soup.find_all('div', class_='result')
            
            for result in search_results[:8]:
                try:
                    link_elem = result.find('a', class_='result__a')
                    snippet_elem = result.find('a', class_='result__snippet')
                    
                    if link_elem:
                        url = link_elem.get('href', '')
                        title = self.clean_text(link_elem.get_text())
                        snippet = self.clean_text(snippet_elem.get_text()) if snippet_elem else ""
                        
                        results.append({
                            'url': url,
                            'title': title,
                            'snippet': snippet
                        })
                except Exception as e:
                    logger.debug(f"Error parsing DDG result: {e}")
                    continue
            
            return results
            
        except Exception as e:
            logger.error(f"DuckDuckGo search error: {e}")
            return []

    async def search_bing(self, query: str) -> List[Dict[str, str]]:
        """Search using Bing as alternative"""
        try:
            search_url = f"https://www.bing.com/search?q={urllib.parse.quote(query)}"
            html = await self.fetch_page(search_url)
            
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            results = []
            
            # Extract Bing search results
            search_results = soup.find_all('li', class_='b_algo')
            
            for result in search_results[:8]:
                try:
                    link_elem = result.find('a')
                    title_elem = result.find('h2')
                    snippet_elem = result.find('p')
                    
                    if link_elem and title_elem:
                        url = link_elem.get('href', '')
                        title = self.clean_text(title_elem.get_text())
                        snippet = self.clean_text(snippet_elem.get_text()) if snippet_elem else ""
                        
                        results.append({
                            'url': url,
                            'title': title,
                            'snippet': snippet
                        })
                except Exception as e:
                    logger.debug(f"Error parsing Bing result: {e}")
                    continue
            
            return results
            
        except Exception as e:
            logger.error(f"Bing search error: {e}")
            return []

    async def search_linkedin(self, query: str) -> str:
        """Search LinkedIn for relevant content using multiple search engines"""
        try:
            # Try different search engines for LinkedIn content
            linkedin_query = f"site:linkedin.com {query}"
            
            # Try DuckDuckGo first
            ddg_results = await self.search_duckduckgo(linkedin_query)
            linkedin_content = []
            
            for result in ddg_results[:3]:
                if result['snippet']:
                    linkedin_content.append(result['snippet'])
            
            # If not enough content, try Bing
            if len(linkedin_content) < 2:
                bing_results = await self.search_bing(linkedin_query)
                for result in bing_results[:3]:
                    if result['snippet'] and result['snippet'] not in linkedin_content:
                        linkedin_content.append(result['snippet'])
            
            return "\n".join(linkedin_content[:5])
            
        except Exception as e:
            logger.error(f"LinkedIn search error: {e}")
            return ""

    async def scrape_url_content(self, url: str) -> str:
        """Scrape content from a specific URL"""
        try:
            html = await self.fetch_page(url)
            if not html:
                return ""
            
            # Common content selectors for different sites
            content_selectors = [
                'article', 'main', '.content', '.post-content', '.entry-content',
                '.article-body', '.story-body', 'p', '.text-content', '.description'
            ]
            
            texts = self.extract_text_from_html(html, content_selectors)
            
            # Combine and limit content
            content = "\n".join(texts[:10])  # Limit to first 10 text blocks
            return content[:2000]  # Limit total length
            
        except Exception as e:
            logger.error(f"Error scraping {url}: {e}")
            return ""

    async def comprehensive_research(self, topic: str) -> (str, list):
        """Perform comprehensive research on a topic using multiple sources and collect URLs"""
        logger.info(f"Starting comprehensive research for: {topic}")
        research_parts = []
        sources = []
        try:
            # Try DuckDuckGo first
            logger.info("Searching with DuckDuckGo...")
            ddg_results = await self.search_duckduckgo(topic)
            if ddg_results:
                ddg_content = []
                for result in ddg_results[:5]:
                    if result['snippet']:
                        ddg_content.append(f"â€¢ {result['title']}: {result['snippet']}")
                        if result['url']:
                            sources.append({'title': result['title'], 'url': result['url']})
                if ddg_content:
                    research_parts.append("Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ:\n" + "\n".join(ddg_content))
            # Try Bing as backup
            if len(research_parts) == 0:
                logger.info("Trying Bing search...")
                bing_results = await self.search_bing(topic)
                if bing_results:
                    bing_content = []
                    for result in bing_results[:5]:
                        if result['snippet']:
                            bing_content.append(f"â€¢ {result['title']}: {result['snippet']}")
                            if result['url']:
                                sources.append({'title': result['title'], 'url': result['url']})
                    if bing_content:
                        research_parts.append("Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ:\n" + "\n".join(bing_content))
            # Search LinkedIn
            logger.info("Searching LinkedIn...")
            linkedin_content = await self.search_linkedin(topic)
            if linkedin_content:
                research_parts.append(f"Ù…Ø­ØªÙˆØ§ÛŒ Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ†:\n{linkedin_content}")
            # Try to scrape some popular sites directly
            logger.info("Trying direct scraping...")
            popular_sites = [
                f"https://fa.wikipedia.org/wiki/{urllib.parse.quote(topic)}",
                f"https://en.wikipedia.org/wiki/{urllib.parse.quote(topic)}"
            ]
            for site_url in popular_sites:
                try:
                    content = await self.scrape_url_content(site_url)
                    if content and len(content) > 100:
                        research_parts.append(f"Ù…Ø­ØªÙˆØ§ÛŒ ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§:\n{content[:800]}")
                        sources.append({'title': 'Wikipedia', 'url': site_url})
                        break
                except:
                    continue
            # If still no content, create basic research from topic
            if not research_parts:
                logger.warning("No external content found, creating basic research")
                basic_research = f"""Ù…ÙˆØ¶ÙˆØ¹ ØªØ­Ù‚ÛŒÙ‚: {topic}\n\nØ§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ø± Ø­ÙˆØ²Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¯Ø§Ø±Ø¯ Ùˆ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø´Ø§Ù…Ù„ Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:\nâ€¢ Ù…ÙØ§Ù‡ÛŒÙ… Ú©Ù„ÛŒØ¯ÛŒ Ùˆ ØªØ¹Ø§Ø±ÛŒÙ Ø§Ø³Ø§Ø³ÛŒ\nâ€¢ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ø¯Ø± ØµÙ†Ø¹Øª\nâ€¢ Ø±ÙˆØ´â€ŒÙ‡Ø§ Ùˆ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·\nâ€¢ ÙÙˆØ§ÛŒØ¯ Ùˆ Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯\nâ€¢ Ø¢ÛŒÙ†Ø¯Ù‡ Ùˆ Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ±Ùˆ\n\nØ¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ØŒ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø± Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ø´ÙˆØ¯."""
                research_parts.append(basic_research)
        except Exception as e:
            logger.error(f"Error in comprehensive research: {e}")
            research_parts.append(f"Ù…ÙˆØ¶ÙˆØ¹: {topic}\nØ¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø§Ù…Ú©Ø§Ù† Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ø§Ø³ØªØŒ Ø§Ù…Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒØŒ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø´Ø§Ù…Ù„ Ù…Ø¨Ø§Ø­Ø« Ù…Ù‡Ù…ÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø¯Ø§Ø±Ø¯.")
        combined_research = "\n\n".join(research_parts)
        logger.info(f"Research completed. Total content length: {len(combined_research)}")
        return combined_research, sources

class MetisAPI:
    def __init__(self, api_key: str, model: str = "gpt-4o"):
        self.api_key = api_key
        self.model = model
        self.api_url = API_URL_METIS

    @retry(
        retry=retry_if_exception_type(RetryableError),
        wait=wait_fixed(RETRY_WAIT_SECONDS),
        stop=stop_after_attempt(RETRY_ATTEMPTS)
    )
    async def generate_posts(self, session: aiohttp.ClientSession, topic: str, research_content: str) -> List[str]:
        """Generate educational posts using Metis API"""
        if len(research_content) > 3000:
            research_content = research_content[:3000] + "..."
        system_prompt = """ØªÙˆ ÛŒÚ© ØªÙˆÙ„ÛŒØ¯Ú©Ù†Ù†Ø¯Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ù‡Ø³ØªÛŒ. ÙˆØ¸ÛŒÙÙ‡â€ŒØ§Øª:\n1. Ø¯Ùˆ Ù¾Ø³Øª Ø¬Ø°Ø§Ø¨ Ùˆ Ù…ÙÛŒØ¯ Ø¨Ù†ÙˆÛŒØ³\n2. Ù„Ø­Ù† Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ø³Ø§Ø¯Ù‡ Ø¨Ø§Ø´Ù‡  \n3. Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†\n4. Ù‡Ø± Ù¾Ø³Øª Ø­Ø¯Ø§Ú©Ø«Ø± 250 Ú©Ù„Ù…Ù‡\n5. Ø§Ú¯Ø± Ù…Ù†Ø§Ø¨Ø¹ ÛŒØ§ Ù„ÛŒÙ†Ú© Ù…ÙÛŒØ¯ÛŒ Ø¯Ø§Ø±ÛŒØŒ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù‡Ø± Ù¾Ø³Øª Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† 'Ù…Ù†Ø§Ø¨Ø¹:' Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª Ù„ÛŒØ³Øª Ù„ÛŒÙ†Ú© Ø¨Ø¯Ù‡\n6. Ù…Ø«Ø§Ù„ ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨ÛŒØ§Ø±\n7. Ø®Ø±ÙˆØ¬ÛŒ Ø±Ùˆ Ø·ÙˆØ±ÛŒ Ø¨Ù†ÙˆÛŒØ³ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø± ØªÙ„Ú¯Ø±Ø§Ù… Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ùˆ Ø¬Ø°Ø§Ø¨ Ø¨Ø§Ø´Ù‡\n8. Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø´Ø¯ØŒ Ø®Ø±ÙˆØ¬ÛŒ Ø±Ùˆ Ø¨Ù‡ Ú†Ù†Ø¯ Ø¨Ø®Ø´ ØªÙ‚Ø³ÛŒÙ… Ú©Ù† Ú©Ù‡ Ù‡Ø± Ø¨Ø®Ø´ Ú©Ù…ØªØ± Ø§Ø² Û´Û°Û°Û° Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¨Ø§Ø´Ù‡ Ùˆ Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ú©Ù†\n"""
        user_prompt = f"""Ù…ÙˆØ¶ÙˆØ¹: {topic}\n\nØ§Ø·Ù„Ø§Ø¹Ø§Øª: {research_content}\n\nØ¯Ùˆ Ù¾Ø³Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ù†ÙˆÛŒØ³:\nÙ¾Ø³Øª Ø§ÙˆÙ„: Ù…Ø¹Ø±ÙÛŒ Ú©Ù„ÛŒ Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ø§ Ù…Ø«Ø§Ù„\nÙ¾Ø³Øª Ø¯ÙˆÙ…: Ù†Ú©Ø§Øª Ø¹Ù…Ù„ÛŒ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨Ø§ Ù…Ø«Ø§Ù„\nÙ‡Ø± Ù¾Ø³Øª Ø±Ùˆ Ø¨Ø§ [Ù¾Ø³Øª Û±] ÛŒØ§ [Ù¾Ø³Øª Û²] Ø´Ø±ÙˆØ¹ Ú©Ù†. Ø§Ú¯Ø± Ù…Ù†Ø§Ø¨Ø¹ Ø¯Ø§Ø±ÛŒØŒ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù‡Ø± Ù¾Ø³Øª Ù„ÛŒØ³Øª Ú©Ù†."""
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": 1000,
            "temperature": 0.7,
            "top_p": 1,
            "frequency_penalty": 0,
            "presence_penalty": 0
        }
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "User-Agent": "Telegram-Bot/1.0"
        }
        try:
            logger.info(f"Sending request to Metis API...")
            logger.debug(f"Payload: {json.dumps(payload, ensure_ascii=False)[:500]}...")
            async with session.post(
                self.api_url, 
                headers=headers, 
                json=payload, 
                timeout=45,
                ssl=False
            ) as response:
                logger.info(f"Metis API response status: {response.status}")
                response_text = await response.text()
                logger.debug(f"Full Metis API response: {response_text}")
                if response.status == 401:
                    logger.error("Authentication failed - check API key")
                    raise RetryableError("Ú©Ù„ÛŒØ¯ API Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
                elif response.status == 403:
                    logger.error("Access forbidden - check permissions")
                    raise RetryableError("Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø³Ø±ÙˆÛŒØ³ Ù…ØªÛŒØ³ Ù…Ø­Ø¯ÙˆØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
                elif response.status >= 500:
                    logger.error(f"Server error: {response.status}")
                    raise RetryableError(f"Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ± Ù…ØªÛŒØ³. Ù„Ø·ÙØ§ Ø¨Ø¹Ø¯Ø§ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯. ({response.status})")
                elif response.status != 200:
                    logger.error(f"Unexpected status: {response.status}, Response: {response_text}")
                    raise RetryableError(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø§Ø² Ù…ØªÛŒØ³: {response.status}")
                try:
                    data = json.loads(response_text)
                    logger.debug(f"Parsed JSON successfully: {data}")
                except json.JSONDecodeError as e:
                    logger.error(f"JSON decode error: {e}, Response: {response_text}")
                    raise RetryableError("Ù¾Ø§Ø³Ø® Ø¯Ø±ÛŒØ§ÙØªÛŒ Ø§Ø² Ù…ØªÛŒØ³ Ù‚Ø§Ø¨Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† Ù†Ø¨ÙˆØ¯.")
                # Defensive: check for choices and content
                if isinstance(data, dict) and 'choices' in data and len(data['choices']) > 0:
                    content = data['choices'][0].get('message', {}).get('content', None)
                    if not content:
                        logger.error(f"No 'content' in Metis response: {data}")
                        raise RetryableError("Ù¾Ø§Ø³Ø® Ù…ØªÛŒØ³ ÙØ§Ù‚Ø¯ Ù…ØªÙ† Ø§Ø³Øª. Ù„Ø·ÙØ§ Ø¨Ø¹Ø¯Ø§ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")
                    logger.info(f"Generated content length: {len(content)}")
                    posts = []
                    if '[Ù¾Ø³Øª Û²]' in content:
                        parts = content.split('[Ù¾Ø³Øª Û²]')
                        if len(parts) == 2:
                            post1 = parts[0].replace('[Ù¾Ø³Øª Û±]', '').strip()
                            post2 = parts[1].strip()
                            posts = [post1, post2]
                    elif '--- Ù¾Ø³Øª Û² ---' in content:
                        parts = content.split('--- Ù¾Ø³Øª Û² ---')
                        if len(parts) == 2:
                            post1 = parts[0].replace('--- Ù¾Ø³Øª Û± ---', '').strip()
                            post2 = parts[1].strip()
                            posts = [post1, post2]
                    if not posts:
                        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
                        if len(paragraphs) >= 4:
                            mid = len(paragraphs) // 2
                            post1 = '\n\n'.join(paragraphs[:mid])
                            post2 = '\n\n'.join(paragraphs[mid:])
                            posts = [post1, post2]
                        else:
                            words = content.split()
                            mid = len(words) // 2
                            post1 = ' '.join(words[:mid])
                            post2 = ' '.join(words[mid:])
                            posts = [post1, post2]
                    valid_posts = [post for post in posts if post.strip() and len(post.strip()) > 50]
                    if not valid_posts:
                        valid_posts = [
                            f"ğŸ“š {topic}\n\n{content}",
                            "ğŸ’¡ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±ØŒ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø± Ø±Ø§ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯."
                        ]
                    logger.info(f"Successfully generated {len(valid_posts)} posts")
                    return valid_posts
                else:
                    logger.error(f"No choices or content in response: {data}")
                    raise RetryableError("Ù¾Ø§Ø³Ø® Ù…ØªÛŒØ³ Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª ÛŒØ§ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§ Ø¨Ø¹Ø¯Ø§ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")
        except aiohttp.ClientError as e:
            logger.error(f"Network error: {e}")
            raise RetryableError(f"Ø®Ø·Ø§ÛŒ Ø´Ø¨Ú©Ù‡ ÛŒØ§ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù…ØªÛŒØ³: {e}")
        except RetryableError:
            raise
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            raise RetryableError(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")

class TelegramBot:
    def __init__(self):
        self.scraper = None
        self.metis_api = MetisAPI(METIS_API_KEY, METIS_MODEL)
        
        # Inline keyboard
        self.menu = InlineKeyboardMarkup([
            [InlineKeyboardButton("ğŸ“ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯", callback_data='new')],
            [InlineKeyboardButton("â“ Ø±Ø§Ù‡Ù†Ù…Ø§", callback_data='help')],
            [InlineKeyboardButton("ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡", callback_data='advanced')],
            [InlineKeyboardButton("âŒ Ù„ØºÙˆ", callback_data='cancel')]
        ])

    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle /start command"""
        user_id = update.effective_user.id
        logger.info(f"User {user_id} started the bot")
        
        welcome_message = """ğŸ¤– Ø³Ù„Ø§Ù…! Ù…Ù† Ø±Ø¨Ø§Øª ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ù‡Ø³ØªÙ…

ğŸ”¥ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ù†:
â€¢ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ØªÙ…Ø§Ù… Ø§ÛŒÙ†ØªØ±Ù†Øª
â€¢ ØªØ­Ù‚ÛŒÙ‚ Ø¯Ø± Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ†
â€¢ ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
â€¢ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø°Ø§Ø¨ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ

âœ¨ Ú©Ø§ÙÛŒÙ‡ Ù…ÙˆØ¶ÙˆØ¹ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±ØªÙˆÙ† Ø±Ùˆ Ø¨ÙØ±Ø³ØªÛŒÙ†!"""
        
        await update.message.reply_text(welcome_message, reply_markup=self.menu)

    async def button_handler(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle inline keyboard buttons"""
        query = update.callback_query
        await query.answer()
        
        user_id = query.from_user.id
        action = query.data
        
        logger.info(f"User {user_id} pressed button: {action}")
        
        if action == 'new':
            message = "ğŸ“ Ù„Ø·ÙØ§ Ù…ÙˆØ¶ÙˆØ¹ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯:\n\n" \
                     "Ù…Ø«Ø§Ù„: Ù…Ø¯ÛŒØ±ÛŒØª ÙØ±ÙˆØ´ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"
            await query.edit_message_text(message, reply_markup=self.menu)
            
        elif action == 'help':
            help_text = """ğŸ“š Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡:

1ï¸âƒ£ Ù…ÙˆØ¶ÙˆØ¹ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
2ï¸âƒ£ ØµØ¨Ø± Ú©Ù†ÛŒØ¯ ØªØ§ ØªØ­Ù‚ÛŒÙ‚ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯
3ï¸âƒ£ Ø¯Ùˆ Ù¾Ø³Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯

ğŸ” Ø±Ø¨Ø§Øª Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø²ÛŒØ± ØªØ­Ù‚ÛŒÙ‚ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:
â€¢ Ú¯ÙˆÚ¯Ù„ (10 Ù†ØªÛŒØ¬Ù‡ Ø§ÙˆÙ„)
â€¢ Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ†
â€¢ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±

â± Ø²Ù…Ø§Ù† ØªØ­Ù‚ÛŒÙ‚: 30-60 Ø«Ø§Ù†ÛŒÙ‡"""
            await query.edit_message_text(help_text, reply_markup=self.menu)
            
        elif action == 'advanced':
            advanced_text = """ğŸ”¬ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡:

Ø¨Ø±Ø§ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ØªØ±ØŒ Ù…ÙˆØ¶ÙˆØ¹ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯:

âœ… Ø®ÙˆØ¨: "Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ø¯ÛŒØ¬ÛŒØªØ§Ù„ Ø¨Ø±Ø§ÛŒ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±Ù‡Ø§ÛŒ Ú©ÙˆÚ†Ú©"
âŒ Ø¨Ø¯: "Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ"

âœ… Ø®ÙˆØ¨: "Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ ÙØ±ÙˆØ´ Ø¢Ù†Ù„Ø§ÛŒÙ†"
âŒ Ø¨Ø¯: "ÙØ±ÙˆØ´"

ğŸ’¡ Ù†Ú©ØªÙ‡: Ù‡Ø±Ú†Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¨Ø§Ø´Ø¯ØŒ Ù…Ø­ØªÙˆØ§ÛŒ Ø¨Ù‡ØªØ±ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯!"""
            await query.edit_message_text(advanced_text, reply_markup=self.menu)
            
        else:  # cancel
            await query.edit_message_text("âŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù„ØºÙˆ Ø´Ø¯.", reply_markup=self.menu)

    async def handle_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle user messages (topics)"""
        user_id = update.effective_user.id
        topic = update.message.text.strip()
        logger.info(f"User {user_id} requested topic: {topic}")
        if len(topic) < 3:
            await update.message.reply_text(
                "âš ï¸ Ù„Ø·ÙØ§ Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ø­Ø¯Ø§Ù‚Ù„ 3 Ú©Ø§Ø±Ø§Ú©ØªØ±)",
                reply_markup=self.menu
            )
            return
        status_message = await update.message.reply_text("ğŸ” Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù‚ÛŒÙ‚... Ù„Ø·ÙØ§ ØµØ¨Ø± Ú©Ù†ÛŒØ¯")
        try:
            await update.message.chat.send_action(ChatAction.TYPING)
            timeout = aiohttp.ClientTimeout(total=60)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                self.scraper = ContentScraper(session)
                await status_message.edit_text("ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø§ÛŒÙ†ØªØ±Ù†Øª...")
                research_content, sources = await self.scraper.comprehensive_research(topic)
                if not research_content:
                    await status_message.edit_text(
                        "âŒ Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ù†ØªÙˆØ§Ù†Ø³ØªÙ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù…. Ù„Ø·ÙØ§ Ù…ÙˆØ¶ÙˆØ¹ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯.",
                        reply_markup=self.menu
                    )
                    return
                await status_message.edit_text("ğŸ¤– Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ...")
                try:
                    posts = await self.metis_api.generate_posts(session, topic, research_content)
                    if not posts:
                        await status_message.edit_text(
                            "âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§. Ù„Ø·ÙØ§ Ù…Ø¬Ø¯Ø¯Ø§ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.",
                            reply_markup=self.menu
                        )
                        return
                    await status_message.delete()
                    # Split and send posts with respect to Telegram's character limit
                    all_posts = []
                    for i, post in enumerate(posts, 1):
                        split_posts = self.split_telegram_messages(post)
                        for idx, chunk in enumerate(split_posts):
                            all_posts.append((i, idx+1, chunk))
                    # If sources exist, format and append to last message
                    if sources:
                        sources_text = self.format_sources(sources)
                        if len(all_posts) > 0:
                            last = all_posts[-1]
                            if len(last[2]) + len(sources_text) < 4096:
                                all_posts[-1] = (last[0], last[1], last[2] + '\n\n' + sources_text)
                            else:
                                all_posts.append((last[0], last[1]+1, sources_text))
                    # Limit to 3 messages
                    all_posts = all_posts[:3]
                    for i, idx, chunk in all_posts:
                        await update.message.chat.send_action(ChatAction.TYPING)
                        await asyncio.sleep(1)
                        await update.message.reply_text(f"ğŸ“ Ù¾Ø³Øª {i} ({idx}):\n\n{chunk}")
                    await update.message.reply_text(
                        "âœ… ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!\n\nğŸ’¡ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ØŒ Ø¯Ú©Ù…Ù‡ Ø²ÛŒØ± Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯:",
                        reply_markup=self.menu
                    )
                except RetryableError as e:
                    logger.error(f"RetryableError in generate_posts: {e}")
                    await status_message.edit_text(
                        "âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø³Ø±ÙˆÛŒØ³ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ. Ù„Ø·ÙØ§ Ù…Ø¬Ø¯Ø¯Ø§ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.",
                        reply_markup=self.menu
                    )
                except Exception as e:
                    logger.error(f"Exception in generate_posts: {e}")
                    try:
                        fallback_posts = self.create_fallback_posts(topic, research_content)
                        await status_message.delete()
                        for i, post in enumerate(fallback_posts, 1):
                            split_posts = self.split_telegram_messages(post)
                            for idx, chunk in enumerate(split_posts, 1):
                                await update.message.chat.send_action(ChatAction.TYPING)
                                await asyncio.sleep(1)
                                await update.message.reply_text(f"ğŸ“ Ù¾Ø³Øª {i} ({idx}):\n\n{chunk}")
                        if sources:
                            sources_text = self.format_sources(sources)
                            await update.message.reply_text(sources_text)
                        await update.message.reply_text(
                            "âš ï¸ Ù…Ø­ØªÙˆØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù‚ÛŒÙ‚Ø§Øª ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯ (Ø¨Ø¯ÙˆÙ† Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ)\n\nğŸ’¡ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ØŒ Ø¯Ú©Ù…Ù‡ Ø²ÛŒØ± Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯:",
                            reply_markup=self.menu
                        )
                    except Exception as fallback_error:
                        logger.error(f"Fallback also failed: {fallback_error}")
                        await status_message.edit_text(
                            "âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡. Ù„Ø·ÙØ§ Ù…Ø¬Ø¯Ø¯Ø§ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.",
                            reply_markup=self.menu
                        )
                except Exception as e:
                    logger.error(f"Exception in handle_message: {e}")
                    await status_message.edit_text(
                        "âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡. Ù„Ø·ÙØ§ Ù…Ø¬Ø¯Ø¯Ø§ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.",
                        reply_markup=self.menu
                    )
        except Exception as e:
            logger.error(f"Exception in handle_message: {e}")
            await status_message.edit_text(
                "âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡. Ù„Ø·ÙØ§ Ù…Ø¬Ø¯Ø¯Ø§ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.",
                reply_markup=self.menu
            )

    def create_fallback_posts(self, topic: str, research_content: str) -> List[str]:
        """Create posts without AI when API fails"""
        
        # Extract key points from research
        lines = research_content.split('\n')
        key_points = []
        
        for line in lines:
            line = line.strip()
            if line and len(line) > 30 and not line.startswith('http'):
                # Clean and format the line
                if line.startswith('â€¢'):
                    key_points.append(line)
                elif ':' in line and len(line.split(':')[1].strip()) > 20:
                    key_points.append(f"â€¢ {line}")
                elif len(line) > 50:
                    key_points.append(f"â€¢ {line}")
        
        # Limit to most relevant points
        key_points = key_points[:8]
        
        # Create first post (introduction)
        post1 = f"""ğŸ“š {topic}

ğŸ” {topic} ÛŒÚ©ÛŒ Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ù‡Ù… Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¯Ø± Ø¯Ù†ÛŒØ§ÛŒ Ø§Ù…Ø±ÙˆØ² Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

ğŸ’¡ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ:
{chr(10).join(key_points[:4] if key_points else ['â€¢ Ù…ÙˆØ¶ÙˆØ¹ÛŒ Ù¾Ø±Ú©Ø§Ø±Ø¨Ø±Ø¯ Ùˆ Ù…ÙÛŒØ¯', 'â€¢ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ ØªØ­Ù‚ÛŒÙ‚ Ø¨ÛŒØ´ØªØ±', 'â€¢ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¯Ø± ØµÙ†Ø§ÛŒØ¹ Ù…Ø®ØªÙ„Ù'])}

ğŸ¯ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ùˆ Ø¯Ø³ØªÛŒØ§Ø¨ÛŒ Ø¨Ù‡ Ø§Ù‡Ø¯Ø§Ù Ú©Ù…Ú© Ú©Ù†Ø¯."""

        # Create second post (practical tips)
        post2 = f"""âš¡ Ù†Ú©Ø§Øª Ø¹Ù…Ù„ÛŒ {topic}

ğŸš€ Ø¨Ø±Ø§ÛŒ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø§ÛŒÙ† Ø­ÙˆØ²Ù‡:

{chr(10).join(key_points[4:] if len(key_points) > 4 else ['â€¢ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±', 'â€¢ ØªÙ…Ø±ÛŒÙ† Ùˆ ØªÚ©Ø±Ø§Ø± Ù…Ø¯Ø§ÙˆÙ…', 'â€¢ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨', 'â€¢ Ù‡Ù…Ú©Ø§Ø±ÛŒ Ø¨Ø§ Ù…ØªØ®ØµØµØ§Ù†'])}

ğŸ’ª ØªÙ†Ù‡Ø§ Ø¨Ø§ Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù† Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡ Ù†ØªØ§ÛŒØ¬ Ù…Ø·Ù„ÙˆØ¨ Ø±Ø³ÛŒØ¯!

#Ø¢Ù…ÙˆØ²Ø´ #{topic.replace(' ', '_')}"""

        return [post1, post2]

    def split_telegram_messages(self, text: str) -> list:
        """Split a long text into Telegram-sized messages (max 4096 chars, up to 3)"""
        max_len = 4096
        chunks = [text[i:i+max_len] for i in range(0, len(text), max_len)]
        return chunks[:3]

    def format_sources(self, sources: list) -> str:
        """Format sources as a neat list of links"""
        if not sources:
            return ''
        lines = ["\n\nğŸ“š Ù…Ù†Ø§Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:"]
        for s in sources:
            if s['url']:
                lines.append(f"ğŸ”— {s['title']}: {s['url']}")
        return '\n'.join(lines)

def main():
    """Main function to run the bot"""
    logger.info("Starting Telegram bot...")
    
    # Create bot instance
    bot = TelegramBot()
    
    # Create application
    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    
    # Add handlers
    application.add_handler(CommandHandler('start', bot.start_command))
    application.add_handler(CallbackQueryHandler(bot.button_handler))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, bot.handle_message))
    
    # Run the bot
    logger.info("Bot is running...")
    application.run_polling(drop_pending_updates=True)

if __name__ == '__main__':
    main()
